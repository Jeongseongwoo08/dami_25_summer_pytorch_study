import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM

def get_long_response():
    model_path = "./qwen_finetuned"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path, torch_dtype=torch.bfloat16, device_map="auto"
    )
    model.eval()

    test_questions = [
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë„ˆë¬´ ì¢‹ì€ë° ì‚°ì±…í•˜ê¸° ì¢‹ì€ ì¥ì†Œ ì¶”ì²œí•´ì¤„ë˜?",
        "ì·¨ì—… ì¤€ë¹„ ë•Œë¬¸ì— ë„ˆë¬´ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ëŠ”ë° ì¡°ì–¸ ì¢€ í•´ì¤˜.",
        "ë¼ë©´ ë§›ìˆê²Œ ë“ì´ëŠ” ë¹„ë²•ì„ ì˜†ì§‘ ì–¸ë‹ˆì²˜ëŸ¼ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì¤˜.",
        "ë¶€ì¥ë‹˜ê»˜ ë³´ë‚¼ ì •ì¤‘í•œ íœ´ê°€ ì‹ ì²­ ë©”ì¼ ì´ˆì•ˆì„ ì‘ì„±í•´ì¤˜."
    ]

    print("\nğŸš€ [í•™ìŠµëœ ëª¨ë¸ì˜ ì „ì²´ ë‹µë³€ ì¶œë ¥]")
    print("=" * 80)

    for q in test_questions:
        messages = [{"role": "user", "content": q}]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        with torch.no_grad():
            out = model.generate(
                **inputs, 
                max_new_tokens=400, # ë‹µë³€ ê¸¸ì´ë¥¼ ì¶©ë¶„íˆ ëŠ˜ë ¸ìŠµë‹ˆë‹¤.
                do_sample=True, 
                temperature=0.7,
                repetition_penalty=1.1 # ê°™ì€ ë§ ë°˜ë³µ ë°©ì§€
            )
        
        full_text = tokenizer.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
        
        # <think> íƒœê·¸ ë‚´ë¶€ ë‚´ìš© ì‚­ì œ í›„ ë³¸ë¡ ë§Œ ì¶”ì¶œ
        cleaned_text = re.sub(r'<think>.*?</think>', '', full_text, flags=re.DOTALL).strip()

        print(f"Q: {q}")
        print(f"A: {cleaned_text}") # [:60]ì„ ì œê±°í•˜ì—¬ ì „ì²´ ë¬¸ì¥ì´ ë‚˜ì˜µë‹ˆë‹¤!
        print("-" * 80)

if __name__ == "__main__":
    get_long_response()